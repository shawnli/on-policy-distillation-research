# On-Policy Distillation 技术原理深度解析

**作者**: Manus AI
**日期**: 2025年12月07日

## 引言

On-Policy Distillation（在策略蒸馏，简称 OPD）是一种先进的大语言模型（LLM）训练技术，旨在高效地将一个强大的“教师”模型的能力迁移到一个小型的“学生”模型中。该技术巧妙地融合了强化学习（Reinforcement Learning, RL）与监督微调（Supervised Fine-Tuning, SFT）的优点，解决了传统方法中存在的诸多挑战。本文将深入探讨 On-Policy Distillation 的核心技术原理、关键机制及其显著优势。

## 传统方法的困境

在理解 OPD 之前，我们首先需要审视两种主流的模型训练与微调方法的局限性。

### 监督微调 (Supervised Fine-Tuning, SFT)

SFT，尤其是基于知识蒸馏（Knowledge Distillation）的 SFT，是一种 **Off-Policy（离策略）** 的方法。它依赖一个固定的、由教师模型生成的高质量数据集来训练学生模型。学生模型通过模仿教师的输出（例如，文本序列或词元概率分布）来进行学习。

- **优点**: 监督信号是 **密集的（Dense）**。学生在训练的每一步都能得到明确的指导，学习效率相对较高。
- **缺点**: 存在 **分布不匹配（Distribution Mismatch）** 问题。学生模型学习的是教师模型的数据分布，但在实际推理时，它会根据自身的参数生成序列。一旦学生犯了一个教师从未犯过的错误，它就会进入一个未知的、未经训练的状态空间，导致后续错误不断累积，即 **复合误差（Compounding Error）** [1]。

### 强化学习 (Reinforcement Learning, RL)

RL 是一种 **On-Policy（在策略）** 的方法。学生模型通过与环境（或一个奖励模型）互动来学习。它根据自己生成的行为（即推理路径）获得奖励或惩罚，并据此调整策略。

- **优点**: 解决了分布不匹配问题。因为模型是根据 **自身** 的行为和错误进行学习，所以能学会如何从错误中恢复。
- **缺点**: 奖励信号通常是 **稀疏的（Sparse）**。例如，在解决一个数学问题时，模型可能只有在最终答案正确或错误时才能获得一个单一的奖励信号。它无法知道是哪一个推理步骤出了问题，这使得学习过程非常低效，正如 Andrej Karpathy 所比喻的，如同“通过一根吸管吮吸监督信号”[2]。

一个生动的类比如下表所示，它清晰地对比了这两种传统方法与 OPD 的区别：

| 方法 | 学习方式类比 | 核心问题 |
| :--- | :--- | :--- |
| **SFT (Off-Policy)** | 学员观看教练开车的录像，并记录模仿。 | 学员不知道当自己开错时该如何纠正。 |
| **RL (On-Policy)** | 学员自己开车，最终到达目的地才被告知开得好不好。 | 学员不知道具体哪个驾驶动作是错误的。 |
| **OPD (On-Policy)** | 学员自己开车，教练坐在副驾，对每一个错误动作进行实时指导。 | 结合了前两者的优点，高效且切中要害。 |

## On-Policy Distillation 的核心机制

On-Policy Distillation 的核心思想是：**让学生模型在自己的数据分布上（On-Policy），接收来自教师模型的密集、细粒度的监督信号（Distillation）** [3]。

其基本流程如下：

1.  **学生自主探索 (Student Rollout)**: 学生模型根据给定的提示（Prompt）自主生成一个完整的文本序列（Trajectory）。
2.  **教师逐词评分 (Token-level Grading)**: 强大的教师模型会评估学生生成的这个序列。关键在于，它不是给整个序列打一个总分，而是对序列中的 **每一个词元（Token）** 进行“评分”或“修正”。
3.  **计算损失并更新 (Loss Calculation & Update)**: 根据教师模型在每个词元上给出的反馈，计算出一个损失值，然后用这个损失来更新学生模型的参数。

通过这种方式，OPD 既能确保学生在自己可能遇到的状态空间中学习（On-Policy 的优势），又能为每一步行为提供明确的优化方向（SFT 的密集信号优势）。

## 关键技术组件

### 1. 损失函数：逆向 KL 散度 (Reverse KL Divergence)

OPD 的一个关键实现是使用 **逆向 KL 散度** 作为损失函数。其目标是最小化学生模型的输出分布 `π_student` 与教师模型的输出分布 `π_teacher` 之间的 KL 散度：

> **Loss = KL(π_student || π_teacher)**

这个选择至关重要，因为它具有 **“模式寻求”（Mode-Seeking）** 的特性。这意味着损失函数会强烈地“惩罚”学生模型在教师模型认为概率很低的区域分配概率，从而迫使学生模型去精确匹配教师模型输出分布中的某个高概率模式（即某种特定的、高质量的思考方式或回答风格），而不是试图覆盖所有可能的模式而导致行为平庸化 [3]。这种特性有效地减少了 **暴露偏差（Exposure Bias）**，并使学生学会更像专家的思考方式。

### 2. 广义知识蒸馏 (Generalized Knowledge Distillation, GKD)

由 Agarwal 等人在 ICLR 2024 的论文中提出的 GKD 框架 [1]，为 OPD 提供了一个更通用和系统的表述。GKD 将传统知识蒸馏视为一个在策略模仿学习问题，其核心贡献在于：

- **形式化框架**: 它将 OPD 形式化，允许在学生自生成的序列上应用来自教师的反馈。
- **灵活性**: GKD 框架不局限于单一的损失函数，可以灵活地采用多种损失（如 f-divergence），这在学生模型表现力不足以完全模仿教师时尤为重要。
- **RLHF 整合**: GKD 可以无缝地与基于人类反馈的强化学习（RLHF）相结合，进一步提升模型性能。

### 3. 跨分词器蒸馏 (Cross-Tokenizer Distillation)

一个实际的挑战是，当教师和学生模型使用不同的分词器（Tokenizer）时，它们生成的词元序列无法直接对齐，导致无法计算损失。Hugging Face 团队提出的 **General On-Policy Logit Distillation (GOLD)** 方法通过以下两步解决了这个问题 [4]：

1.  **跨分词器序列对齐**: 将不同分词器生成的序列在字符串级别进行对齐。
2.  **Logprob 合并**: 将对齐后的词元概率（Logprobs）进行合并，从而计算出有效的蒸馏损失。

这使得 OPD 技术可以应用于任意模型组合，极大地扩展了其实用性。

## OPD 为何如此高效？

知乎文章中提到的“形”与“神”的类比非常精辟地解释了 OPD 高效的原因 [2]。

- **传统蒸馏学“形”**: 学生模型学习的是教师遣词造句的 **表面特征**。它知道在什么上下文后应该接什么词，但并不真正理解背后的逻辑。
- **OPD 学“神”**: 学生模型学习的是教师的 **语义策略（Semantic Strategies）** 或 **思考过程**。因为它是在自己的错误轨迹上被纠正，所以它能学会“如果我这样想，就会出错；我应该像老师那样思考”。这是一种更深层次的认知模式迁移。

## 实践优势

1.  **极高的训练效率**: Thinking Machines Lab 的实验表明，SFT 需要 200 万个 prompts 才能达到的效果，OPD 仅需 7.7 万个即可，效率提升超过 25 倍 [3]。
2.  **缓解灾难性遗忘**: 在持续学习新知识时，模型原有的能力（如遵循指令）会下降。实验发现，在新知识训练后，使用原始模型作为教师进行一轮 OPD，可以恢复模型被遗忘的能力，同时新知识也基本不受影响。这表明，知识和行为可能存储在模型参数的不同部分，而 OPD 恰好能精准地调整与行为相关的参数。

## 结论

On-Policy Distillation 是一种强大而高效的模型后训练（Post-training）范式。它通过让学生模型在自我探索中接收教师模型的密集、即时反馈，成功地结合了强化学习和监督微调的优点，克服了分布不匹配和奖励稀疏的核心难题。通过学习教师的“思考策略”而非仅仅是“语言风格”，OPD 不仅极大地提升了训练效率，还在持续学习等领域展现出巨大潜力，为训练更小、更强、更专业的语言模型开辟了新的道路。

---

### 参考文献

[1] Agarwal, R., et al. (2024). *On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes*. In International Conference on Learning Representations (ICLR).

[2] wangleineo. (2025). *Thinking Machines新文章：On-Policy Distillation*. 知乎想法. [https://www.zhihu.com/pin/1968462515513062544](https://www.zhihu.com/pin/1968462515513062544)

[3] Lu, K. (2025). *On-Policy Distillation*. Thinking Machines Lab Blog. [https://thinkingmachines.ai/blog/on-policy-distillation/](https://thinkingmachines.ai/blog/on-policy-distillation/)

[4] Patino, C. M., et al. (2025). *Unlocking On-Policy Distillation for Any Model Family*. Hugging Face Blog. [https://huggingface.co/spaces/HuggingFaceH4/on-policy-distillation](https://huggingface.co/spaces/HuggingFaceH4/on-policy-distillation)
