# On-Policy Distillation 代码使用指南

本指南介绍如何使用本仓库中收集的代码实现来进行 On-Policy Distillation 训练。

## 方案选择

根据您的需求和资源情况，可以选择以下两种主要实现：

| 实现方案 | 适用场景 | 优势 | 劣势 |
|---------|---------|------|------|
| **Hugging Face TRL** | 开源项目、研究实验 | 完全开源、社区支持强、易于上手 | 需要自行配置训练环境 |
| **Thinking Machines Lab Tinker** | 商业应用、大规模训练 | 托管式训练、自动分布式优化 | 需要申请 API 访问权限 |

---

## 方案一：使用 Hugging Face TRL

### 1. 环境准备

```bash
# 克隆 TRL 仓库（已包含在本仓库中）
cd trl

# 安装依赖
pip install -e .

# 或直接安装发布版本
pip install trl
```

### 2. 使用 GKDTrainer（Generalized Knowledge Distillation）

GKDTrainer 是 ICLR 2024 论文中提出的方法的实现。

#### 基础示例

```python
from trl import GKDTrainer, GKDConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# 加载教师模型
teacher_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")

# 加载学生模型
student_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B")

# 加载分词器
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")

# 加载数据集
dataset = load_dataset("trl-lib/Capybara", split="train")

# 配置训练参数
config = GKDConfig(
    output_dir="./gkd_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=5e-5,
)

# 创建训练器
trainer = GKDTrainer(
    model=student_model,
    teacher_model=teacher_model,
    args=config,
    train_dataset=dataset,
    processing_class=tokenizer,
)

# 开始训练
trainer.train()
```

#### 运行示例脚本

```bash
# 使用 TRL 提供的示例脚本
cd trl/examples/scripts
python gkd.py \
    --model_name_or_path "meta-llama/Llama-3.2-1B" \
    --teacher_model_name_or_path "meta-llama/Llama-3.2-3B-Instruct" \
    --dataset_name "trl-lib/Capybara" \
    --output_dir "./gkd_output"
```

### 3. 使用 GOLD Trainer（General On-Policy Logit Distillation）

GOLD 支持跨分词器蒸馏，适用于教师和学生使用不同分词器的场景。

```python
from trl.experimental.gold import GOLDTrainer, GOLDConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# 加载不同分词器的模型
teacher_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
student_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B")

# 加载对应的分词器
teacher_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
student_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")

dataset = load_dataset("trl-lib/Capybara", split="train")

config = GOLDConfig(
    output_dir="./gold_output",
    num_train_epochs=3,
)

trainer = GOLDTrainer(
    model=student_model,
    teacher_model=teacher_model,
    args=config,
    train_dataset=dataset,
    processing_class=student_tokenizer,
    teacher_processing_class=teacher_tokenizer,
)

trainer.train()
```

### 4. 使用 MiniLLM Trainer

MiniLLM 是 Thinking Machines Lab 方法的泛化版本，支持更多自定义选项。

```python
from trl.experimental.minillm import MiniLLMTrainer, MiniLLMConfig

config = MiniLLMConfig(
    output_dir="./minillm_output",
    num_train_epochs=3,
    # 可以添加分布级别的蒸馏信号
    use_distribution_distillation=True,
)

trainer = MiniLLMTrainer(
    model=student_model,
    teacher_model=teacher_model,
    args=config,
    train_dataset=dataset,
    processing_class=tokenizer,
)

trainer.train()
```

---

## 方案二：使用 Thinking Machines Lab Tinker

### 1. 获取访问权限

1. 访问 [Tinker 官网](https://thinkingmachines.ai/tinker/) 申请 waitlist
2. 获得访问权限后，从 [控制台](https://console.thinkingmachines.ai/) 创建 API Key
3. 设置环境变量：
   ```bash
   export TINKER_API_KEY="your-api-key-here"
   ```

### 2. 安装 Tinker SDK

```bash
pip install tinker
```

### 3. 基础使用示例

```python
import tinker

# 创建服务客户端
service_client = tinker.ServiceClient()

# 创建 LoRA 训练客户端
training_client = service_client.create_lora_training_client(
    base_model="meta-llama/Llama-3.2-1B",
    rank=32,
)

# 执行训练步骤
training_client.forward_backward(...)
training_client.optim_step(...)

# 保存模型
training_client.save_state(...)

# 创建采样客户端用于推理
sampling_client = training_client.save_weights_and_get_sampling_client(
    name="my_distilled_model"
)
```

### 4. 使用 Tinker Cookbook

Tinker Cookbook 提供了更高级的抽象和完整示例。

```bash
cd tinker-cookbook

# 查看可用的训练配方
ls tinker_cookbook/recipes/

# 运行基础监督学习示例
python tinker_cookbook/recipes/sl_basic.py

# 运行强化学习示例
python tinker_cookbook/recipes/rl_basic.py
```

---

## 关键参数说明

### 学习率 (Learning Rate)

- **GKD/GOLD**: 建议 `5e-5` 到 `1e-4`
- **MiniLLM**: 建议 `1e-4` 到 `5e-4`

### 批次大小 (Batch Size)

- 根据 GPU 内存调整
- 推荐使用梯度累积来模拟更大的批次
- 典型值：`per_device_train_batch_size=4`, `gradient_accumulation_steps=4`

### 训练轮数 (Epochs)

- On-Policy Distillation 通常收敛较快
- 建议 1-3 个 epoch
- 使用验证集监控性能，避免过拟合

### 温度参数 (Temperature)

- 控制教师模型输出分布的平滑度
- 默认值：`1.0`
- 较高的温度（如 `2.0`）可以提供更平滑的目标分布

---

## 评估与验证

### 使用 TRL 内置评估

```python
# 在训练配置中启用评估
config = GKDConfig(
    output_dir="./output",
    evaluation_strategy="steps",
    eval_steps=500,
    per_device_eval_batch_size=8,
)

# 提供验证数据集
trainer = GKDTrainer(
    model=student_model,
    teacher_model=teacher_model,
    args=config,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    processing_class=tokenizer,
)
```

### 使用标准基准测试

```bash
# 安装评估工具
pip install lm-eval

# 评估模型
lm_eval --model hf \
    --model_args pretrained=./gkd_output \
    --tasks gsm8k,mmlu \
    --device cuda:0 \
    --batch_size 8
```

---

## 常见问题

### Q1: 内存不足怎么办？

**解决方案**:
- 使用梯度检查点：`gradient_checkpointing=True`
- 减小批次大小并增加梯度累积步数
- 使用 LoRA 或 QLoRA 进行参数高效训练
- 使用 DeepSpeed 或 FSDP 进行分布式训练

### Q2: 如何选择教师模型？

**建议**:
- 教师模型应在目标任务上表现优异
- 教师模型通常比学生模型大 2-10 倍
- 可以使用经过 RLHF 训练的指令模型作为教师

### Q3: 训练多久才能看到效果？

**经验**:
- On-Policy Distillation 通常在几千到几万个样本后就能看到明显改进
- 相比传统 SFT，所需数据量可减少 10-30 倍
- 建议每 500 步评估一次，观察性能曲线

---

## 参考资源

- [TRL 官方文档](https://huggingface.co/docs/trl)
- [GKD 论文](https://arxiv.org/abs/2306.13649)
- [Thinking Machines Lab 博客](https://thinkingmachines.ai/blog/on-policy-distillation/)
- [Hugging Face 论坛讨论](https://discuss.huggingface.co/)

---

## 下一步

1. 根据您的任务选择合适的数据集
2. 选择教师和学生模型组合
3. 从小规模实验开始，验证配置
4. 逐步扩大训练规模
5. 在目标任务上评估性能

祝您训练顺利！
